{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Block which represents residual connection\n",
        "    with 2-x convolution layers, instance normalization and relu activation between it.\n",
        "    Instance normalization and relu activation on outputs is applied.\n",
        "    ...\n",
        "    Attributes\n",
        "    ----------\n",
        "    instance_norm: torch.nn.modules.instancenorm.InstanceNorm2d\n",
        "        Layer which applies normalization on each channel separately for each object in sample(batch)\n",
        "    relu: torch.nn.modules.activation.ReLU\n",
        "        Applies ReLU activation\n",
        "    conv_block: torch.nn.modules.container.Sequential\n",
        "        Block which contains convolutional1 -> instance norm -> relu -> convolutional2\n",
        "    ...\n",
        "    Methods\n",
        "    -------\n",
        "    forward:\n",
        "        Forward pass of a model\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_channels: int):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        # self.instance_norm = nn.InstanceNorm2d(n_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv_block = nn.Sequential(*[\n",
        "            nn.Conv2d(\n",
        "                in_channels=n_channels,\n",
        "                out_channels=n_channels,\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=1\n",
        "            ),\n",
        "            nn.InstanceNorm2d(n_channels),\n",
        "            nn.Dropout(0.5, inplace=True),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(\n",
        "                in_channels=n_channels,\n",
        "                out_channels=n_channels,\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=1\n",
        "            ),\n",
        "            nn.InstanceNorm2d(n_channels),\n",
        "        ])\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        output = self.conv_block(x) + x\n",
        "        output = self.relu(output)\n",
        "        return output\n",
        "\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    \"\"\"\n",
        "    Convolutional u-net-like generator model.\n",
        "    It has downsample part of 3 convolutional layers, n_residuals residual layers and\n",
        "    upsample part of 2 transposed convolutional layers and 1 convolutional layer.\n",
        "    ...\n",
        "    Attributes\n",
        "    ----------\n",
        "    reflection_pad: torch.nn.modules.padding.ReflectionPad2d\n",
        "        Makes reflection pad by 3 on each side\n",
        "    relu: torch.nn.modules.activation.ReLU\n",
        "        Applies ReLU activation\n",
        "    pixel_shuffle: torch.nn.modules.pixelshuffle.PixelShuffle\n",
        "        Rearrange pixels by upscale factor == 2, output: (*, channel / factor ** 2, height * factor, width * factor)\n",
        "    gathered_layers: list\n",
        "        All the layers in generator model\n",
        "    generator_block: torch.nn.modules.container.Sequential\n",
        "        All the layers in generator model wrapped in torch.nn.Sequential instance, used in forward method\n",
        "    ...\n",
        "    Methods\n",
        "    -------\n",
        "    gather_layers:\n",
        "        Makes list of all layers in generator, it is used to create generator block of model\n",
        "    forward:\n",
        "        Forward pass of a model\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 in_n_channels: int,\n",
        "                 hidden_n_channels: int,\n",
        "                 n_residuals: int = 9):\n",
        "        super(Generator, self).__init__()\n",
        "        self.reflection_pad = nn.ReflectionPad2d(3)\n",
        "        self.pixel_shuffle = nn.PixelShuffle(2)  # not used for now\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.gathered_layers = self.gather_layers(in_n_channels, hidden_n_channels, n_residuals)\n",
        "        self.generator_block = nn.Sequential(*self.gathered_layers)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        output = self.generator_block(x)\n",
        "        return output\n",
        "\n",
        "    def gather_layers(self,\n",
        "                      in_n_channels: int,\n",
        "                      hidden_n_channels: int,\n",
        "                      n_residuals: int):\n",
        "        layers = []\n",
        "        # shape: bc * 3 * 256 * 256\n",
        "        layers += [\n",
        "            self.reflection_pad,\n",
        "            # shape: bc * 3 * 262 * 262\n",
        "            nn.Conv2d(\n",
        "                in_channels=in_n_channels,\n",
        "                out_channels=hidden_n_channels,  # for example 64\n",
        "                kernel_size=7,\n",
        "                stride=1,\n",
        "                padding=0,\n",
        "            ),\n",
        "            # shape: bc * hc * 256 * 256\n",
        "            nn.InstanceNorm2d(hidden_n_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Conv2d(\n",
        "                in_channels=hidden_n_channels,\n",
        "                out_channels=2 * hidden_n_channels,\n",
        "                kernel_size=3,\n",
        "                stride=2,\n",
        "                padding=1,\n",
        "            ),\n",
        "            # shape: bc * 2hc * 128 * 128\n",
        "            nn.InstanceNorm2d(2 * hidden_n_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Conv2d(\n",
        "                in_channels=2 * hidden_n_channels,\n",
        "                out_channels=4 * hidden_n_channels,\n",
        "                kernel_size=3,\n",
        "                stride=2,\n",
        "                padding=1,\n",
        "            ),\n",
        "            # shape: bc * 4hc * 64 * 64\n",
        "            nn.InstanceNorm2d(4 * hidden_n_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "        ]\n",
        "\n",
        "        for _ in range(n_residuals):\n",
        "            layers.append(ResidualBlock(4 * hidden_n_channels))\n",
        "            # shape: bc * 4hc * 64 * 64\n",
        "\n",
        "        layers += [\n",
        "            nn.ConvTranspose2d(\n",
        "                in_channels=4 * hidden_n_channels,\n",
        "                out_channels=2 * hidden_n_channels,\n",
        "                kernel_size=3,\n",
        "                stride=2,\n",
        "                padding=1,\n",
        "                output_padding=1,\n",
        "            ),\n",
        "            # # shape: bc * 8hc * 64 * 64\n",
        "            # self.pixel_shuffle,\n",
        "            # shape: bc * 2hc * 128 * 128\n",
        "            nn.InstanceNorm2d(2 * hidden_n_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.ConvTranspose2d(\n",
        "                in_channels=2 * hidden_n_channels,\n",
        "                out_channels=hidden_n_channels,\n",
        "                kernel_size=3,\n",
        "                stride=2,\n",
        "                padding=1,\n",
        "                output_padding=1,\n",
        "            ),\n",
        "            # # shape: bc * 4hc * 128 * 128\n",
        "            # self.pixel_shuffle,\n",
        "            # shape: bc * hc * 256 * 256\n",
        "            nn.InstanceNorm2d(hidden_n_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            # shape: bc * hc * 262 * 262\n",
        "            nn.Conv2d(\n",
        "                in_channels=hidden_n_channels,\n",
        "                out_channels=in_n_channels,\n",
        "                kernel_size=7,\n",
        "                stride=1,\n",
        "                padding=3,\n",
        "                padding_mode=\"reflect\"),  # The same as using nn.ReflectionPad2d(3) as layer\n",
        "            # shape: bc * 3 * 256 * 256\n",
        "            nn.Tanh()\n",
        "        ]\n",
        "        return layers\n",
        "\n",
        "\n",
        "def test_generator() -> None:\n",
        "    x = torch.rand(2, 3, 256, 256)\n",
        "    generator = Generator(3, 64)\n",
        "    output = generator(x)\n",
        "    print(\"Output shape:\", output.shape)\n",
        "    return None\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    test_generator()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iiUhLyRN8-K9",
        "outputId": "a4692847-b7a9-4b33-d77b-95f223965af7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: torch.Size([2, 3, 256, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QB79mPX59eu6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}